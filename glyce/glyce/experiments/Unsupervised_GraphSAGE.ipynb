{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Unsupervised GraphSAGE 300",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ6CBSZ6l-36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e47f10c-bc93-406b-8c4f-260491f2c597"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuBcSQa7KwR1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd2b5da-adc2-414d-f2d3-35ba31160423"
      },
      "source": [
        "!pip install pypinyin pywubi zhconv overrides boto3\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
        "!pip install torch-geometric"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pypinyin\n",
            "  Downloading pypinyin-0.42.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.1 MB/s \n",
            "\u001b[?25hCollecting pywubi\n",
            "  Downloading pywubi-0.0.2-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 38.3 MB/s \n",
            "\u001b[?25hCollecting zhconv\n",
            "  Downloading zhconv-1.4.2.tar.gz (183 kB)\n",
            "\u001b[K     |████████████████████████████████| 183 kB 39.9 MB/s \n",
            "\u001b[?25hCollecting overrides\n",
            "  Downloading overrides-6.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.18.6-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 42.6 MB/s \n",
            "\u001b[?25hCollecting typing-utils>=0.0.3\n",
            "  Downloading typing_utils-0.1.0-py3-none-any.whl (10 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.22.0,>=1.21.6\n",
            "  Downloading botocore-1.21.6-py3-none-any.whl (7.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.7 MB 33.4 MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.22.0,>=1.21.6->boto3) (2.8.1)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.6->boto3) (1.15.0)\n",
            "Building wheels for collected packages: zhconv\n",
            "  Building wheel for zhconv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zhconv: filename=zhconv-1.4.2-py2.py3-none-any.whl size=181082 sha256=c56b65ff5f0a65d7df910cea686c927a56f2f8a58a625837eac1d45fa3dbfab1\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/31/84/fca23def9be1db201eeaa76f4ee50a7d64f6e20ee7b223cc4f\n",
            "Successfully built zhconv\n",
            "Installing collected packages: urllib3, jmespath, botocore, typing-utils, s3transfer, zhconv, pywubi, pypinyin, overrides, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.6 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.18.6 botocore-1.21.6 jmespath-0.10.0 overrides-6.1.0 pypinyin-0.42.0 pywubi-0.0.2 s3transfer-0.5.0 typing-utils-0.1.0 urllib3-1.26.6 zhconv-1.4.2\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_scatter-2.0.7-cp37-cp37m-linux_x86_64.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 7.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.7\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_sparse-0.6.10-cp37-cp37m-linux_x86_64.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.10\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-cluster\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_cluster-1.5.9-cp37-cp37m-linux_x86_64.whl (926 kB)\n",
            "\u001b[K     |████████████████████████████████| 926 kB 7.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-cluster\n",
            "Successfully installed torch-cluster-1.5.9\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html\n",
            "Collecting torch-spline-conv\n",
            "  Downloading https://pytorch-geometric.com/whl/torch-1.9.0%2Bcu102/torch_spline_conv-1.2.1-cp37-cp37m-linux_x86_64.whl (382 kB)\n",
            "\u001b[K     |████████████████████████████████| 382 kB 8.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-spline-conv\n",
            "Successfully installed torch-spline-conv-1.2.1\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-1.7.2.tar.gz (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 8.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.5.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.15)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Collecting rdflib\n",
            "  Downloading rdflib-6.0.0-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.2.0)\n",
            "Collecting isodate\n",
            "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.0.1)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-1.7.2-py3-none-any.whl size=388142 sha256=b19f298a7973a052af570f0add34598d7b83eb458e15a020a72dcdbbe62e1ffb\n",
            "  Stored in directory: /root/.cache/pip/wheels/55/93/b6/2eeb0465afe89aee74d7a07a606e9770466d7565abd45a99d5\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: urllib3, isodate, rdflib, torch-geometric\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.6\n",
            "    Uninstalling urllib3-1.26.6:\n",
            "      Successfully uninstalled urllib3-1.26.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed isodate-0.6.0 rdflib-6.0.0 torch-geometric-1.7.2 urllib3-1.25.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_m9AcVEKGUi"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_cluster import random_walk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import SAGEConv\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import NeighborSampler as RawNeighborSampler\n",
        "import pickle\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNvTvkVcSo9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4703cee-0aff-476b-e24f-8e8cd1faddb4"
      },
      "source": [
        "print(torch.__version__)\n",
        "print(torch.cuda.get_device_name(0))\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "start = time.perf_counter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.9.0+cu102\n",
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKDT0gvuKA2v"
      },
      "source": [
        "EPS = 1e-15\n",
        "\n",
        "class NeighborSampler(RawNeighborSampler):\n",
        "    def sample(self, batch):\n",
        "        batch = torch.tensor(batch)\n",
        "        row, col, _ = self.adj_t.coo()\n",
        "\n",
        "        # For each node in `batch`, we sample a direct neighbor (as positive\n",
        "        # example) and a random node (as negative example):\n",
        "        pos_batch = random_walk(row, col, batch, walk_length=1,\n",
        "                                coalesced=False)[:, 1]\n",
        "\n",
        "        neg_batch = torch.randint(0, self.adj_t.size(1), (batch.numel(), ),\n",
        "                                  dtype=torch.long)\n",
        "\n",
        "        batch = torch.cat([batch, pos_batch, neg_batch], dim=0)\n",
        "        return super(NeighborSampler, self).sample(batch)\n",
        "\n",
        "class SAGE(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels):\n",
        "        super(SAGE, self).__init__()\n",
        "        self.gconv1 = SAGEConv(in_channels,hidden_channels)\n",
        "        self.gconv2 = SAGEConv(hidden_channels, hidden_channels)\n",
        "\n",
        "    def forward(self, x, adjs):\n",
        "        for i, (edge_index, _, size) in enumerate(adjs):\n",
        "            x_target = x[:size[1]]  # Target nodes are always placed first.\n",
        "            if i==0:\n",
        "                x = self.gconv1((x, x_target), edge_index)\n",
        "                x = x.relu()\n",
        "                x = F.dropout(x,p=0.5,training=self.training)\n",
        "            else:\n",
        "                x = self.gconv2((x, x_target), edge_index)\n",
        "        return x\n",
        "\n",
        "    def full_forward(self, x, edge_index):\n",
        "        x = self.gconv1(x,edge_index)\n",
        "        x = x.relu()\n",
        "        x = F.dropout(x,p=0.5,training=self.training)\n",
        "        x = self.gconv2(x,edge_index)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq5rhmBwNUBW"
      },
      "source": [
        "def train(data,model,train_loader,optimizer):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for batch_size, n_id, adjs in train_loader:\n",
        "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\n",
        "        adjs = [adj.to(device) for adj in adjs]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = model(x[n_id], adjs)\n",
        "        out, pos_out, neg_out = out.split(out.size(0) // 3, dim=0)\n",
        "\n",
        "        pos_loss = F.logsigmoid((out * pos_out).sum(-1)).mean()\n",
        "        neg_loss = F.logsigmoid(-(out * neg_out).sum(-1)).mean()\n",
        "        loss = -pos_loss - neg_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += float(loss) * out.size(0)\n",
        "\n",
        "    return total_loss / data.num_nodes\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(data,model,x,edge_index):\n",
        "    model.eval()\n",
        "    out = model.full_forward(x, edge_index).cpu()\n",
        "\n",
        "    clf = LogisticRegression()\n",
        "    clf.fit(out[data.train_mask], data.y[data.train_mask])\n",
        "\n",
        "    val_acc = clf.score(out[data.val_mask], data.y[data.val_mask])\n",
        "    test_acc = clf.score(out[data.test_mask], data.y[data.test_mask])\n",
        "\n",
        "    return val_acc, test_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR1yG0vUMqy8"
      },
      "source": [
        "def getGraphEmbeddings(graphPath,numFeat):\n",
        "    myGraphs = []\n",
        "    with open(graphPath,\"rb\") as f:\n",
        "        graphs = pickle.load(f)\n",
        "        for zi in graphs:\n",
        "            myGraphs.append(graphs[zi].to(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "    return myGraphs\n",
        "myGraphs = getGraphEmbeddings(\"/content/gdrive/MyDrive/Colab Data/Chinese Characters/graphsDictOrder.pickle\",6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqSWChPHM9cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f83097d2-536f-43b9-dd90-1e1308d88d6e"
      },
      "source": [
        "model = SAGE(6,hidden_channels=300).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)\n",
        "for epoch in range(1, 2):\n",
        "    for i,data in enumerate(myGraphs):\n",
        "        x, edge_index = data.x.to(device).float(), data.edge_index.to(device)\n",
        "        train_loader = NeighborSampler(edge_index,sizes=[10, 10],batch_size=256,shuffle=True,num_nodes=data.num_nodes)\n",
        "        for miniEpoch in range(1,11):    \n",
        "            loss = train(data,model,train_loader,optimizer)\n",
        "        if i%100==0:\n",
        "            print(\"Step %04d/%04d: %.4f\" % (i,len(myGraphs),loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0000/9574: 8429.4375\n",
            "Step 0100/9574: 1445.5065\n",
            "Step 0200/9574: 2.2389\n",
            "Step 0300/9574: 1.4107\n",
            "Step 0400/9574: 1.3150\n",
            "Step 0500/9574: 1.3640\n",
            "Step 0600/9574: 1.3671\n",
            "Step 0700/9574: 1.4199\n",
            "Step 0800/9574: 5.3347\n",
            "Step 0900/9574: 203.6454\n",
            "Step 1000/9574: 1.5638\n",
            "Step 1100/9574: 1.3962\n",
            "Step 1200/9574: 1.3814\n",
            "Step 1300/9574: 1.3632\n",
            "Step 1400/9574: 1.4433\n",
            "Step 1500/9574: 1.3787\n",
            "Step 1600/9574: 1.7091\n",
            "Step 1700/9574: 1.3993\n",
            "Step 1800/9574: 1.3997\n",
            "Step 1900/9574: 1.4525\n",
            "Step 2000/9574: 1.3876\n",
            "Step 2100/9574: 6.9155\n",
            "Step 2200/9574: 1.7520\n",
            "Step 2300/9574: 2.0483\n",
            "Step 2400/9574: 1.3390\n",
            "Step 2500/9574: 1.3878\n",
            "Step 2600/9574: 9.0914\n",
            "Step 2700/9574: 1.3877\n",
            "Step 2800/9574: 26.9066\n",
            "Step 2900/9574: 1.3533\n",
            "Step 3000/9574: 1.3928\n",
            "Step 3100/9574: 1.3859\n",
            "Step 3200/9574: 1.2710\n",
            "Step 3300/9574: 1.3875\n",
            "Step 3400/9574: 1.4184\n",
            "Step 3500/9574: 1.3986\n",
            "Step 3600/9574: 2.4010\n",
            "Step 3700/9574: 405.6048\n",
            "Step 3800/9574: 1.4118\n",
            "Step 3900/9574: 1.3961\n",
            "Step 4000/9574: 1.3024\n",
            "Step 4100/9574: 1.3866\n",
            "Step 4200/9574: 1.3605\n",
            "Step 4300/9574: 1.4094\n",
            "Step 4400/9574: 8.9870\n",
            "Step 4500/9574: 1.3896\n",
            "Step 4600/9574: 1.3868\n",
            "Step 4700/9574: 1.3812\n",
            "Step 4800/9574: 1.3879\n",
            "Step 4900/9574: 1.3870\n",
            "Step 5000/9574: 1.4611\n",
            "Step 5100/9574: 1.3864\n",
            "Step 5200/9574: 1.5075\n",
            "Step 5300/9574: 1.3969\n",
            "Step 5400/9574: 1.3866\n",
            "Step 5500/9574: 1693.0366\n",
            "Step 5600/9574: 1.3869\n",
            "Step 5700/9574: 1.3734\n",
            "Step 5800/9574: 1.3863\n",
            "Step 5900/9574: 1.3863\n",
            "Step 6000/9574: 1.3863\n",
            "Step 6100/9574: 1.3863\n",
            "Step 6200/9574: 6.4340\n",
            "Step 6300/9574: 1.3886\n",
            "Step 6400/9574: 1.3934\n",
            "Step 6500/9574: 1.3876\n",
            "Step 6600/9574: 1.3863\n",
            "Step 6700/9574: 1.3907\n",
            "Step 6800/9574: 1.3868\n",
            "Step 6900/9574: 1.3864\n",
            "Step 7000/9574: 1.3870\n",
            "Step 7100/9574: 1.3864\n",
            "Step 7200/9574: 1.3874\n",
            "Step 7300/9574: 1.3864\n",
            "Step 7400/9574: 1.5034\n",
            "Step 7500/9574: 1.3996\n",
            "Step 7600/9574: 1.3864\n",
            "Step 7700/9574: 1.3864\n",
            "Step 7800/9574: 1.5537\n",
            "Step 7900/9574: 1.5540\n",
            "Step 8000/9574: 1.8458\n",
            "Step 8100/9574: 1.3874\n",
            "Step 8200/9574: 1.3873\n",
            "Step 8300/9574: 1.3863\n",
            "Step 8400/9574: 1.3863\n",
            "Step 8500/9574: 1.3863\n",
            "Step 8600/9574: 1.4599\n",
            "Step 8700/9574: 1.3863\n",
            "Step 8800/9574: 1.3869\n",
            "Step 8900/9574: 1.3881\n",
            "Step 9000/9574: 1.3870\n",
            "Step 9100/9574: 1.3863\n",
            "Step 9200/9574: 1.3864\n",
            "Step 9300/9574: 1.3935\n",
            "Step 9400/9574: 1.3863\n",
            "Step 9500/9574: 5.3447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWdK2kq9hYnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c13a2dcf-1ea5-4c79-9c2b-d374abe56f72"
      },
      "source": [
        "torch.save(model.state_dict(),\"/content/gdrive/MyDrive/Colab Data/Chinese Characters/unsupGraphModelDict300.bin\")\n",
        "end = time.perf_counter()\n",
        "elapsed = end-start\n",
        "hours = elapsed//(60*60)\n",
        "mins = (elapsed - hours*60*60)//60\n",
        "secs = (elapsed - hours*60*60 - mins*60)\n",
        "print(\"Time elapsed: %02d:%02d:%02d\" % (hours,mins,secs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time elapsed: 00:07:14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6WlsncFm631"
      },
      "source": [
        "class MyGCN(nn.Module):\n",
        "    def __init__(self,layer,num_features,hidden,output_features,gcn_drop,k,pool):\n",
        "        super(MyGCN, self).__init__()\n",
        "        self.gcn_drop = gcn_drop\n",
        "        self.k = k\n",
        "        self.pool = pool\n",
        "\n",
        "        if layer==\"SAGE\":\n",
        "            self.gconv1 = SAGEConv(num_features,hidden)\n",
        "            self.gconv2 = SAGEConv(hidden, hidden)\n",
        "        elif layer==\"GCN\":\n",
        "            self.gconv1 = GCNConv(num_features,hidden)\n",
        "            self.gconv2 = GCNConv(hidden,hidden)\n",
        "\n",
        "        self.conv1d = nn.Conv1d(hidden, 32, 5)\n",
        "        self.linear1 = nn.Linear(32 * (self.k - 5 + 1), hidden)\n",
        "        self.linear2 = nn.Linear(hidden, output_features)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = self.gconv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.gcn_drop, training=self.training)\n",
        "        x = self.gconv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        if self.pool==\"sort\":\n",
        "            x = global_sort_pool(x, batch, self.k)\n",
        "            x = x.view(len(x), self.k, -1).permute(0, 2, 1)\n",
        "            x = F.relu(self.conv1d(x))\n",
        "            x = x.view(len(x), -1)\n",
        "            x = F.relu(self.linear1(x))\n",
        "            x = F.dropout(x, p=self.gcn_drop, training=self.training)\n",
        "            x = self.linear2(x)\n",
        "            x = F.sigmoid(x)\n",
        "        elif self.pool==\"max\":\n",
        "            x = global_max_pool(x, batch)\n",
        "            x = F.dropout(x, p=self.gcn_drop, training=self.training)\n",
        "            x = self.linear2(x)\n",
        "            x = F.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnpzgthcm9yh",
        "outputId": "2b6a2faa-b24a-47af-973e-cb842634eae3"
      },
      "source": [
        "gcn = MyGCN(\"SAGE\",6,300,24,0.5,30,\"sort\")\n",
        "gcn.load_state_dict(torch.load(\"/content/gdrive/MyDrive/Colab Data/Chinese Characters/unsupGraphModelDict300.bin\"),strict=False)\n",
        "gcn.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyGCN(\n",
              "  (gconv1): SAGEConv(6, 300)\n",
              "  (gconv2): SAGEConv(300, 300)\n",
              "  (conv1d): Conv1d(300, 32, kernel_size=(5,), stride=(1,))\n",
              "  (linear1): Linear(in_features=832, out_features=300, bias=True)\n",
              "  (linear2): Linear(in_features=300, out_features=24, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}